{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Discretized Streams(DStreams) ? \n",
    "\n",
    "DStreams are the basic abstraction provided by Spark streaming. DStream represent a continuous stream of data either the input data stream received from the source or the proceess that data stream generated by transforming the input stream. \n",
    "\n",
    "Internally, DStream is represented by a continuous series of RDD(resilient distributed datasets), which is Spark's abstraction of an immutable distributed dataset. Each RDD in a DStream contains data from certain interval as shown in the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'image/Discretized_Stream.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any operation applied on a DStream translates to operations on the underlying RDDs. For example, in the figure representing DStream and its underlying RDDs the flatmap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream. These underlying RDD transformation are computed by the Spark engine and DStream provide the developer with a higher level API for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'image/Discretize_Analogy.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose you're the owner of a coffee shop. Many people suddenly come together and order same kind of coffee. If you have some of battistas in the shop, you don't need to make those one by one. The order can be allocated by the battisats and made much faster than making all of those by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is RDDs ###\n",
    "\n",
    "RDD(Resilient Distributed Data) is literally distributed immutable java-object collection. It make calculation a lot quicker and it's the core of Apache Spark.\n",
    "\n",
    "Dataset is distributed. Dataset is seperated by a key as bunch of chunk and it's divided as executor node. So, the running time could be faster. RDD is also used to trace back all the transformation applied to each chuck. It make the computation quick and resolve any problem on the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
